{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ea4a9c",
   "metadata": {},
   "source": [
    "# Resampling\n",
    "We have been talking about breaking the data into train/test data sets and cross-validation as a way to avoid overfitting a model to the data. In this exercise, we will try a few different ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f541c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our most-used packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Import useful ones from the sklearn package \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c294a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for plotting\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2856f812",
   "metadata": {},
   "source": [
    "## The Data\n",
    "We have been given sample data from our customers. The data has been aggregated from individual purchases / transactions across the time period (e.g., last year). The goal is to see if we can predict the spending amounts for the next time period (e.g., next year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data and print out its shape\n",
    "cust = pd.read_csv('./data/customers_clean.csv')\n",
    "print(cust.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac64a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at info\n",
    "cust.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the data\n",
    "cust.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead4548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .value_counts is useful for categorical columns\n",
    "cust.value_counts(subset=['gender', 'marital_status', 'home_ownership'],\n",
    "                 normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca672c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See summary statistics\n",
    "cust.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07e23d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use .describe() on all columns\n",
    "# This can be useful to see if we might need to clean the data\n",
    "cust.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f159e",
   "metadata": {},
   "source": [
    "## End Result for Input\n",
    "We want to have all numerical variables. This means we should create *dummy* variables for `gender`, `marital_status`, and `home_ownership`. We also do not need `cust_id` since it is just a unique id (now that we have dropped duplicates). The two date columns could be used to create numerical values, but we will simply ignore them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac1144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to create dummy variables for gender, marital_status,\n",
    "# and home_ownership\n",
    "pd.get_dummies(cust[['gender','marital_status','home_ownership']],\n",
    "              dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3286c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop the following columns:\n",
    "# cust_id, join_date, last_purchase_date\n",
    "new_cust = cust.drop(columns=['cust_id','join_date','last_purchase_date'])\n",
    "new_cust.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba538cd",
   "metadata": {},
   "source": [
    "## Create Dummy Variables\n",
    "\n",
    "If your `DataFrame` contains categorical (or object) columns, you can call `pd.get_dummies(your_dataframe)` to create the dummy variables for **every** categorical column in the `DataFrame`. Since we deleted the \"extra\" columns, let's try it on our `new_cust` variable and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all dummies on all categorical columns\n",
    "pd.get_dummies(new_cust, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Really just want k-1 dummies for k categories\n",
    "# We can use the argument drop_first=True\n",
    "pd.get_dummies(new_cust, dtype=int, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it again and save it in a new DataFrame\n",
    "data = pd.get_dummies(new_cust, dtype=int, drop_first=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0502cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at .describe()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d242420",
   "metadata": {},
   "source": [
    "## Some Visualizations\n",
    "\n",
    "Let's try to look at a few visualizations for the attributes that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2eb8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create histograms for each variable in data now\n",
    "data.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc51017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try boxplots?\n",
    "data.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c4679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could loop over every column and make a displot\n",
    "for i in data.columns:\n",
    "    sns.displot(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64cdab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about correlations?\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce89b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatamp of correlations\n",
    "sns.heatmap(data.corr(), vmin=-1, vmax=1, annot=True, linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ee2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a pairplot\n",
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4405157",
   "metadata": {},
   "source": [
    "## Time for Resampling Attempts\n",
    "Need to first define `X` and `y`. Then we can try train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefcd138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the output variable, y\n",
    "y = data.spend\n",
    "\n",
    "# define the X\n",
    "X = data.drop('spend', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af40add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at shape of X\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at shape of y\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=163)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf8bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at shape of X_train\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a76ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at shape of X_test\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd192c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick out summary statistics for X_train\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ced4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick out summary statistics for X_test\n",
    "# Hope these are close to X_train stats\n",
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to run a regression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "print(f'Intercept:    {reg.intercept_}')\n",
    "print(f'Coefficients: {reg.coef_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2d55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can compute the training R-squared, MSE, and RMSE\n",
    "trainR2 = r2_score(y_train, reg.predict(X_train))\n",
    "trainMSE = mean_squared_error(y_train, reg.predict(X_train))\n",
    "trainRMSE = np.sqrt(trainMSE)\n",
    "\n",
    "print(f'Training R-squared is: {trainR2:.2%}')\n",
    "print(f'          and MSE is:  {trainMSE:.2f}')\n",
    "print(f'          and RMSE is: {trainRMSE:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a55974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Really interested in the test metrics\n",
    "pred = reg.predict(X_test)\n",
    "rSquare = r2_score(y_test, pred)\n",
    "mse = mean_squared_error(y_test, pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'Test R-squared is: {rSquare:.2%}')\n",
    "print(f'      and MSE is:  {mse:.2f}')\n",
    "print(f'      and RMSE is: {rmse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b2d6ee",
   "metadata": {},
   "source": [
    "## Try Different Splits\n",
    "We said that a different 80/20 split will give us different metrics. We hope that they are not \"too different\". Let's split the full data set 10 different ways and see how the metrics differ. We can also calculate the average MSE, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98527b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random number generator seed \n",
    "np.random.seed(131)\n",
    "\n",
    "# Create 10 different random_state values for splitting\n",
    "random_states = np.random.choice(range(1,500), 10)\n",
    "random_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6794063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's loop through the randomStates, split the data, fit the model\n",
    "# Calculate the metrics (capture them)\n",
    "# Create 6 empty dictionaries\n",
    "trainR2s = {}\n",
    "trainMSEs = {}\n",
    "trainRMSEs = {}\n",
    "testR2s = {}\n",
    "testMSEs = {}\n",
    "testRMSEs = {}\n",
    "\n",
    "# Use our good friend the for loop\n",
    "for i in random_states:\n",
    "    # split the data using randomStates[i] stored in i\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                       random_state=i)\n",
    "    \n",
    "    # Build and fit the regression model\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions for the training set and calculate metrics\n",
    "    trainPred = reg.predict(X_train)\n",
    "    trainR2s[i] = r2_score(y_train, trainPred)\n",
    "    trainMSEs[i] = mean_squared_error(y_train, trainPred)\n",
    "    trainRMSEs[i] = np.sqrt(mean_squared_error(y_train, trainPred))\n",
    "    \n",
    "    # Make predictions for the test set and calculate metrics\n",
    "    testPred = reg.predict(X_test)\n",
    "    testR2s[i] = r2_score(y_test, testPred)\n",
    "    testMSEs[i] = mean_squared_error(y_test, testPred)\n",
    "    testRMSEs[i] = np.sqrt(mean_squared_error(y_test, testPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f63ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out training metric dictionaries\n",
    "print(trainR2s)\n",
    "print(trainMSEs)\n",
    "print(trainRMSEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15eac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make these dictionaries into DataFrames\n",
    "trainingR2s = pd.DataFrame.from_dict(trainR2s, orient='index', columns=['Training_R2'])\n",
    "trainingMSEs = pd.DataFrame.from_dict(trainMSEs, orient='index', columns=['Training_MSE'])\n",
    "trainingRMSEs = pd.DataFrame.from_dict(trainRMSEs, orient='index', columns=['Training_RMSE'])\n",
    "\n",
    "# Combine them into a single DataFrame\n",
    "training_metrics = pd.concat([trainingR2s, trainingMSEs, trainingRMSEs], axis='columns')\n",
    "training_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a802b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metrics"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c6b2d26",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b1b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out test metric dictionaries\n",
    "print(testR2s)\n",
    "print(testMSEs)\n",
    "print(testRMSEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1299bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the test dictionaries into DataFrames\n",
    "testingR2s = pd.DataFrame.from_dict(testR2s, orient='index', columns=['Testing_R2'])\n",
    "testingMSEs = pd.DataFrame.from_dict(testMSEs, orient='index', columns=['Testing_MSE'])\n",
    "testingRMSEs = pd.DataFrame.from_dict(testRMSEs, orient='index', columns=['Testing_RMSE'])\n",
    "\n",
    "# Combine them into a single DataFrame\n",
    "testing_metrics = pd.concat([testingR2s, testingMSEs, testingRMSEs], axis='columns')\n",
    "testing_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the average RMSE for training and test sets\n",
    "print(f'Avg Training RMSE: {training_metrics.Training_RMSE.mean():.2f}')\n",
    "print(f'Avg Test RMSE:     {testing_metrics.Testing_RMSE.mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfa026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=list(range(1,11)),\n",
    "                y=training_metrics.Training_RMSE,\n",
    "                label='Train MSE',\n",
    "                color='blue')\n",
    "sns.scatterplot(x=list(range(1,11)),\n",
    "                y=testing_metrics.Testing_RMSE,\n",
    "                label='Test RMSE',\n",
    "                color='orange')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('Split #')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1351191",
   "metadata": {},
   "source": [
    "# What about Leave-One-Out CV?\n",
    "Can we do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a80e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeaveOneOut really likes numpy arrays\n",
    "# Create numpy arrays out X and y\n",
    "XArray = X.to_numpy()\n",
    "yArray = y.to_numpy()\n",
    "print(XArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e2dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LeaveOneOut object and call get_n_splits\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(XArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57815b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the test RMSEs from loo\n",
    "looRMSEs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train_index, test_index) in enumerate(loo.split(XArray)):\n",
    "    print(f'Fold {i}')\n",
    "    print(f'   Train index = {train_index}')\n",
    "    print(f'   Test index  = {test_index}')\n",
    "    if i == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3de860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all of the splits\n",
    "for i, (train_index, test_index) in enumerate (loo.split(XArray)):\n",
    "    X_train, X_test = XArray[train_index], XArray[test_index]\n",
    "    y_train, y_test = yArray[train_index], yArray[test_index]\n",
    "    # Fit the model\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X_train, y_train)\n",
    "    # make prediction\n",
    "    pred = reg.predict(X_test)\n",
    "    # find RMSE and store it\n",
    "    looRMSEs.append(np.sqrt(mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a7bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the average RMSE\n",
    "np.mean(looRMSEs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26259d",
   "metadata": {},
   "source": [
    "### What if we wanted to replicate LOOCV with cross_val_score?\n",
    "We can do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec3db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what scorers we have for metrics\n",
    "# import all of sklearn (because I am lazy at this point)\n",
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb41fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LinearRegression object\n",
    "# call cross_val_score\n",
    "reg = LinearRegression()\n",
    "cvScores = cross_val_score(reg, X, y, cv=len(X),\n",
    "                          scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3169e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out average RMSE ... should be same as above\n",
    "np.mean(np.sqrt(np.absolute(cvScores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4574e0c",
   "metadata": {},
   "source": [
    "## Try $K$-Fold CV\n",
    "\n",
    "Remember $K$ is the number of folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80dc7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try K=5 fold CV\n",
    "folds = 5\n",
    "reg = LinearRegression()\n",
    "cvMSE = cross_val_score(reg, X, y, cv=folds, scoring='neg_mean_squared_error')\n",
    "cvR2 = cross_val_score(reg, X, y, cv=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out RMSEs\n",
    "print(np.sqrt(np.absolute(cvMSE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out R-squareds\n",
    "print(cvR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d223acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the average RMSE and average R-squared\n",
    "print(f'Avg CV RMSE = {np.mean(np.sqrt(np.absolute(cvMSE))):.2f}')\n",
    "print(f'Avg CV R2   = {np.mean(cvR2):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52725582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use KFold\n",
    "folds = 5\n",
    "kf5 = KFold(n_splits=folds)\n",
    "reg = LinearRegression()\n",
    "cvMSE2 = cross_val_score(reg, X, y, cv=kf5, scoring='neg_mean_squared_error')\n",
    "cvR2_2 = cross_val_score(reg, X, y, cv=kf5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7695310",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'CV RMSE = {np.mean(np.sqrt(np.absolute(cvMSE2))):.2f}')\n",
    "print(f'CV R2   = {np.mean(cvR2_2):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58de129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try shuffling the folds\n",
    "folds = 5\n",
    "# Set shuffle=True\n",
    "kf5 = KFold(n_splits=folds, shuffle=True)\n",
    "reg = LinearRegression()\n",
    "cvMSE3 = cross_val_score(reg, X, y, cv=kf5, scoring='neg_mean_squared_error')\n",
    "cvR2_3 = cross_val_score(reg, X, y, cv=kf5)\n",
    "\n",
    "print(f'CV RMSE = {np.mean(np.sqrt(np.absolute(cvMSE3))):.2f}')\n",
    "print(f'CV R2   = {np.mean(cvR2_3):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb0c57",
   "metadata": {},
   "source": [
    "## How Many Folds?\n",
    "\n",
    "Let's try several different values for $k$, the number of folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49abe156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 dictionaries to hold results\n",
    "avgRMSEs = {}\n",
    "stdRMSEs = {}\n",
    "\n",
    "# Loop over multiple k for the folds storing results\n",
    "for i in range(2, 16):\n",
    "    reg = LinearRegression()\n",
    "    cvMSE = cross_val_score(reg, X, y, cv=i,\n",
    "                           scoring='neg_mean_squared_error')\n",
    "    avgRMSEs[i] = np.mean(np.sqrt(np.absolute(cvMSE)))\n",
    "    stdRMSEs[i] = np.std(np.sqrt(np.absolute(cvMSE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ce1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the average RMSEs and the average std\n",
    "print(avgRMSEs)\n",
    "print(stdRMSEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a2b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the avg MSE for each value of k\n",
    "sns.scatterplot(x=list(avgRMSEs.keys()), y=list(avgRMSEs.values()))\n",
    "plt.xlabel('K used in cross-validation')\n",
    "plt.ylabel('Average RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the standard deviation for each k used\n",
    "sns.scatterplot(x=list(stdRMSEs.keys()), y=list(stdRMSEs.values()))\n",
    "plt.xlabel('K used in cross-validation')\n",
    "plt.ylabel('Standard Deviation of RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea69d53",
   "metadata": {},
   "source": [
    "## What About Predicting\n",
    "\n",
    "You have been given a new list of customers. You want to predict how much they will spend next year. What should we do? The file is in `new_cust.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104b270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ac1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a83162ac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2022 - Present: Matthew D. Dean, Ph.D.   \n",
    "Clinical Associate Professor of Business Analytics at William \\& Mary.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
