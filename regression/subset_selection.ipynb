{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ea4a9c",
   "metadata": {},
   "source": [
    "# Finding Useful Features/Variables/Input\n",
    "\n",
    "A regression model that has fewer input/independent variables is often desirable for simplicity and explainability. It is often referred to as having a **parsimonius** model. See this article for more details on the \"[law of parsimony][1]\".\n",
    "\n",
    "Let's try a few different ways to find the \"best\" model using our customer dataset.\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Occam%27s_razor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f541c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our most-used packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Import useful ones from the sklearn package \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## RFE classes\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "\n",
    "## Other classes for linear models\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import statsmodels stuff\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2856f812",
   "metadata": {},
   "source": [
    "## The Data\n",
    "We have been given sample data from our customers. The data has been aggregated from individual purchases / transactions across the time period (e.g., last year). The goal is to see if we can predict the spending amounts for the next time period (e.g., next year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data and print out its shape\n",
    "cust = pd.read_csv('./data/customers_clean.csv')\n",
    "print(cust.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac64a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at info\n",
    "cust.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the data\n",
    "cust.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca672c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See summary statistics\n",
    "cust.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f159e",
   "metadata": {},
   "source": [
    "## End Result for Input\n",
    "We want to have all numerical variables. This means we should create *dummy* variables for `gender`, `marital_status`, and `home_ownership`. We also do not need `cust_id` since it is just a unique id (now that we have dropped duplicates). The two date columns could be used to create numerical values, but we will simply ignore them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3286c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop the following columns:\n",
    "# cust_id, join_date, last_purchase_date\n",
    "new_cust = cust.drop(columns=['cust_id','join_date','last_purchase_date'])\n",
    "new_cust.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba538cd",
   "metadata": {},
   "source": [
    "## Create Dummy Variables\n",
    "\n",
    "If your `DataFrame` contains categorical (or object) columns, you can call `pd.get_dummies(your_dataframe)` to create the dummy variables for **every** categorical column in the `DataFrame`. Since we deleted the \"extra\" columns, let's try it on our `new_cust` variable and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it again and save it in a new DataFrame\n",
    "data = pd.get_dummies(new_cust, dtype=int, drop_first=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0502cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at .describe()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4405157",
   "metadata": {},
   "source": [
    "## Split Data into Training and Test Sets\n",
    "\n",
    "Need to first define `X` and `y`. Then we can try train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefcd138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the output variable, y\n",
    "y = data.spend\n",
    "\n",
    "# define the X\n",
    "X = data.drop('spend', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=163)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf8bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at shape of X_train\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a76ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at shape of X_test\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd192c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick out summary statistics for X_train\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ced4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick out summary statistics for X_test\n",
    "# Hope these are close to X_train stats\n",
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95efb5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = pd.concat([X_train, y_train], axis='columns')\n",
    "full_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79973cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a full regression model with statsmodels\n",
    "# Create the RHS (the independent variables)\n",
    "rhs = '+'.join([str(col) for col in X_train.columns])\n",
    "print(rhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c407987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full multiple linear regression with statsmodels\n",
    "full_mlr = ols('spend ~ ' + rhs, data=full_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acdbc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mlr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a88e52",
   "metadata": {},
   "source": [
    "## Finding VIFs\n",
    "\n",
    "Unfortunately, the `summary()` method does not include the VIFs for each variable. (Wouldn't that be nice!) We can, however, find them somewhat painlessly. The code cell below contains a function that will compute VIFs for the features/variables that are passed into it using the `X_train` dataset we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the vif for all given features\n",
    "def compute_vif(considered_features):\n",
    "    X = X_train[considered_features]\n",
    "    # the calculation of variance inflation requires a constant\n",
    "    X['intercept'] = 1\n",
    "    \n",
    "    # create dataframe to store vif values\n",
    "    vif = pd.DataFrame()\n",
    "    vif['Variable'] = X.columns\n",
    "    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    vif = vif[vif['Variable']!='intercept']\n",
    "    return vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d163a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the function all the columns from X_train\n",
    "compute_vif(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5879efe",
   "metadata": {},
   "source": [
    "### What Does This Mean?\n",
    "\n",
    "You tell me!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400665fd",
   "metadata": {},
   "source": [
    "## We Should Scale the Data\n",
    "\n",
    "Before we attempt to use RFE, Ridge, Lasso, or ElasticNet, we should scale the data. We will use the `StandardScaler` which makes each variable have a mean of 0 and a standard deviation of 1. It does not change the shape of the data. (Thought Exercise: How would you verify that statement?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6829ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler on just the training X variables\n",
    "# Let's start with StandardScaler which will center\n",
    "# each variable at 0 and give each a unit variance (=1)\n",
    "s_scaler = StandardScaler().fit(X_train)\n",
    "s_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train\n",
    "X_train_ss = s_scaler.transform(X_train)\n",
    "# Put it in a DataFrame using same column names\n",
    "train_X_ss = pd.DataFrame(X_train_ss, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a0a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the DataFrame\n",
    "train_X_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de854cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for scaled data\n",
    "train_X_ss.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0c87a",
   "metadata": {},
   "source": [
    "## RFE \n",
    "\n",
    "Recursive feature elimination will rank the input variables if you tell it to only select a single feature. Let's try it and see what order it thinks the input variables add value to the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run regression on the standardized data (mean=0, stdev=1)\n",
    "mlr = LinearRegression()\n",
    "rfe = RFE(mlr, n_features_to_select=1)\n",
    "\n",
    "mlr.fit(rfe.fit_transform(train_X_ss, y_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642e0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a boolean array called support_\n",
    "# Because we had n_features_to_select=1, only one will be True\n",
    "rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f772d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What order are the other input variables in?\n",
    "rfe.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e659a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to make sense of the ranking by matching each ranking\n",
    "# with its column name\n",
    "print('Input variables sorted by their rank:')\n",
    "\n",
    "d = {}\n",
    "for i in range(len(rfe.ranking_)):\n",
    "    d[rfe.ranking_[i]] = train_X_ss.columns[i]\n",
    "    \n",
    "print(sorted(d.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba719eea",
   "metadata": {},
   "source": [
    "### 'Optimal' Number of Features?\n",
    "\n",
    "Let's try RFECV that is supposed to find the \"optimal\" number of features for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c472cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_rfecv = LinearRegression()\n",
    "rfecv = RFECV(mlr_rfecv)\n",
    "\n",
    "mlr_rfecv.fit(rfecv.fit_transform(train_X_ss, y_train), y_train)\n",
    "print(f'Optimal number of features: {rfecv.n_features_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1413f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the support and ranking\n",
    "print(rfecv.support_)\n",
    "print(rfecv.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a95a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out all the variables that have a ranking of 1\n",
    "for i in range(len(rfecv.ranking_)):\n",
    "    if rfecv.ranking_[i] == 1:\n",
    "        print(f'Ranking of 1: {train_X_ss.columns[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca831294",
   "metadata": {},
   "source": [
    "Notice that those all \"tie\" for a ranking of 1. It does not tell us which of those 5 input variables is the most important similar to what we saw with `RFE` setting `n_features_to_select=1`. In this case, they are simply printed out in the order they appear in the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do the estimated coefficients look like?\n",
    "rfecv.estimator_.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ac350",
   "metadata": {},
   "source": [
    "## Verify Estimated Coefficients\n",
    "\n",
    "Lets' use `statsmodels` to verify the estimated coefficients that we received from the `RFECV` model. We are going to use a slightly different implementation of ordinary least squares than we have used from `statsmodels` before. The reason is that it you can pass in two different datasets, one for the independent variables and one for the dependent variable which makes our lives a bit simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame called new_X_train that contains \"important\" variables\n",
    "new_X_train = train_X_ss[['household_income','gender_M','marital_status_unmarried','home_ownership_rent','home_ownership_unknown']]\n",
    "\n",
    "# We need a constant term with the approach we are taking\n",
    "new_X_train = sm.add_constant(new_X_train)\n",
    "\n",
    "# See what new_X_train looks like\n",
    "new_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4591783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create a DataFrame for the y_train data\n",
    "# Most notably, we are resetting the index so that it\n",
    "# aligns with the index for the X variables\n",
    "train_y = pd.DataFrame(y_train,\n",
    "             columns=['spend']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at it\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65cafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OLS model, fit it, and look at its summary\n",
    "m = sm.OLS(train_y, new_X_train).fit()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8cc5d",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "Let's try Ridge regression (also called the $\\ell_{2}$ norm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ridge with alpha=0\n",
    "# This should create a model with the same\n",
    "# estimated coefficients as the full model\n",
    "ridge = Ridge(alpha=0)\n",
    "ridge.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd669e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the estimated coefficients in a Series for easier viewing\n",
    "pd.Series(ridge.coef_, index=train_X_ss.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab351cb",
   "metadata": {},
   "source": [
    "### Alpha's Effect\n",
    "\n",
    "In general, the larger the value of $\\alpha$, the more penalization (regularization) which will shrink the estimated coefficients. We may have to change the value of alpha several times to see any major changes in the estimated coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74486f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try changing alpha\n",
    "ridge2 = Ridge(alpha=4)\n",
    "ridge2.fit(X_train_ss, y_train)\n",
    "pd.Series(ridge2.coef_, index=train_X_ss.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b46d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(3,1,100)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1331cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha=a)\n",
    "    ridge.fit(X_train_ss, y_train)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "#ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Ridge coefficients as a function of the regularization')\n",
    "plt.legend(train_X_ss.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616726e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558942e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a CV Ridge Regression to find best alpha\n",
    "ridgecv = RidgeCV(alphas=alphas, scoring='neg_mean_squared_error')\n",
    "ridgecv.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26737d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best alpha?\n",
    "ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aafb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Ridge Regression with the found alpha\n",
    "rba = Ridge(alpha=ridgecv.alpha_)\n",
    "rba.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c0a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(rba.coef_, index=train_X_ss.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6546c3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fc56e39",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "\n",
    "Let's try Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter=10000)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas*2:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(X_train_ss, y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas*2, coefs)\n",
    "ax.set_xscale('log')\n",
    "#ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Lasso coefficients as a function of the regularization')\n",
    "plt.legend(train_X_ss.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7850e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Lasso with default alpha\n",
    "lasso1 = Lasso()\n",
    "lasso1.fit(X_train_ss, y_train)\n",
    "lasso1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbaf655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV\n",
    "lassocv = LassoCV(alphas=alphas, max_iter=10000)\n",
    "lassocv.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9c28d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best alpha for LassoCV\n",
    "lassocv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f2390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "lassocv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615b121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a74269af",
   "metadata": {},
   "source": [
    "## ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7432a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ElasticNet using default parameters\n",
    "en = ElasticNet()\n",
    "# fit it\n",
    "en.fit(X_train_ss, y_train)\n",
    "# Look at the estimated coefficients\n",
    "en.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a57e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ElasticNetCV to find best alpha\n",
    "enCV = ElasticNetCV(cv=5, alphas=alphas, max_iter=10000)\n",
    "enCV.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best alpha for elasticnetcv\n",
    "enCV.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be87b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ElasticNet with best alpha\n",
    "en2 = ElasticNet(alpha=enCV.alpha_)\n",
    "en2.fit(X_train_ss, y_train)\n",
    "en2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f591c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the ElasticNet model en2\n",
    "en2.predict(s_scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6251e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the RMSE of that model\n",
    "np.sqrt(mean_squared_error(y_test, en2.predict(s_scaler.transform(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83162ac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2022 - Present: Matthew D. Dean, Ph.D.   \n",
    "Clinical Associate Professor of Business Analytics at William \\& Mary.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
