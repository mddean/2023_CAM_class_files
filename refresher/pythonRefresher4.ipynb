{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "575f5758",
   "metadata": {},
   "source": [
    "# Aggregating Data\n",
    "\n",
    "You will undoubtedly want to aggregate your data in some fashion. You may be given the data in separate files or in various tables in a database. Some of the common aggregation tasks include:\n",
    "\n",
    "- Querying\n",
    "- Merging\n",
    "- Binning\n",
    "- Applying functions\n",
    "- Summarizing\n",
    "- Creating pivot tables and crosstabs\n",
    "\n",
    "We will explore some of these tasks in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242dfc4",
   "metadata": {},
   "source": [
    "## Querying Data\n",
    "\n",
    "You will encounter two basic ways to query data when using `pandas`. One approach is to use a Boolean mask to get the results you want. The other approach is to use the `.query()` function. This function makes it easy to write complicated filters instead of using a Boolean mask. We will examine both approaches using weather data from Williamsburg found in the file `./data/wburg_weather.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613f5ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the cleaned_data from our last section\n",
    "wburg_weather = pd.read_csv('./data/wburg_weather.csv')\n",
    "\n",
    "# Look at its info()\n",
    "wburg_weather.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dbb9a0",
   "metadata": {},
   "source": [
    "We want to find all the hourly data where the wind direction was variable, the dry bulb temperature in Fahrenheit was between 75 and 80 degrees (inclusively), and the station pressure was less than 29.5. To use a Boolean mask, we use the bitwise **and** operator, which is the ampersand character, `&`. We also need to put each condition inside of parentheses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6220f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Boolean mask\n",
    "wburg_weather[(wburg_weather.HOURLYWindDirection == 'Variable Direction')\n",
    "             & (wburg_weather.HOURLYDRYBULBTEMPF >= 75) \n",
    "             & (wburg_weather.HOURLYDRYBULBTEMPF <= 80)\n",
    "             & (wburg_weather.HOURLYStationPressure < 29.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6345c335",
   "metadata": {},
   "source": [
    "When we use the `.query()` method, we can use both logical operators (`and`, `or`, `not`) and bitwise operators (`&`, `|`, `~`). In our case, we can use the logical operator `and`, which makes reading our statement easier. You should be able to verify that the results of both approaches are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c606ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .query() method instead\n",
    "wburg_weather.query('HOURLYWindDirection == \"Variable Direction\" '\n",
    "                   'and HOURLYDRYBULBTEMPF >= 75 and HOURLYDRYBULBTEMPF <= 80 '\n",
    "                   'and HOURLYStationPressure < 29.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa774228",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "Continuing with our weather data, complete the following tasks:\n",
    "\n",
    "1. Using a Boolean mask, find all the rows that have hourly sky conditions of `CLR:00`, a dew point temperature greater than 75, and the relative humidity of either 94 or 100. Save your results in the variable `my_mask`.\n",
    "2. How many rows does `my_mask` have?\n",
    "3. Using the `.query()` method, perform the same task as above. Save the results in the variable `my_query`.\n",
    "4. How many rows does `my_query` have?\n",
    "5. Verify that the two `DataFrame`s contain the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using a Boolean mask, find all the rows that have hourly sky conditions of `CLR:00`,\n",
    "# a dew point temperature greater than 75, and the relative humidity of either 94 or 100.\n",
    "# Save your results in the variable `my_mask`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1361927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. How many rows does `my_mask` have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abff5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Using the `.query()` method, perform the same task as above. \n",
    "# Save the results in the variable `my_query`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00a017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. How many rows does `my_query` have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8819c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Verify that the two `DataFrame`s contain the same data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1121ff64",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "## Binning\n",
    "\n",
    "There are times when you want to take a variable that is continuous (or discrete with a lot of valid values) and create a new variable that creates \"buckets\" (bins or categories). For example, you might be more interested in analyzing the daily high temperature as one of several categories instead of a numerical quantity. Perhaps you think any temperature above 90&deg; F is \"hot\". You could create five different \"buckets\" to classify temperature as \"cold\", \"cool\", \"just right\", \"warm\", or \"hot\". How you decided to discretize a continuous variable is highly dependent on how you plan to use the new variable in your analysis. Here we will explore a couple of ways to bin our data.\n",
    "\n",
    "We will be using the daily stock trading data for the company Apple for the year 2022. The data is in the file `./data/aapl_2022.csv`. Let's read it in trying to take care of the commas in the numerical columns and being sure to create a real date for the `Date` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Apple trading data\n",
    "apple = pd.read_csv('./data/aapl_2022.csv', thousands=',', parse_dates=['Date'])\n",
    "apple.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ba272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all column names to lower case\n",
    "apple.columns = apple.columns.str.lower()\n",
    "apple.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57323a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the index to the date\n",
    "apple.set_index('date', inplace=True)\n",
    "apple.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e6fafa",
   "metadata": {},
   "source": [
    "We are interested in investigating the volume of trading for Apple's stock in the year 2022. In particular, we want to calculate the absolute value of the Z-score for volume. Recall from statistics that the Z-score tells you how far away a particular observation is from the mean of all the observations for that variable. It tells you how many standard deviations away a data point is from the average. If your data follows a Normal distribution, then you would expect approximately 99.7% of the observations to fall between -3 and +3 standard deviations from the mean. Let's see if we have any days of trading volume that are outside of three standard deviations, in absolute terms, for Apple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dad7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the absolute value of Z-score for volume\n",
    "apple = apple.assign(abs_z_score_vol \n",
    "                     = lambda x: x.volume.sub(x.volume.mean()).div(x.volume.std()).abs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which days had volumes outside of 3 standard deviations?\n",
    "apple.query('abs_z_score_vol > 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd2b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine trading days around the May 12th date\n",
    "apple.loc['2022-05-10':'2022-05-16']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d215424",
   "metadata": {},
   "source": [
    "You may also be interested in the percentage change of a variable. You can use the `.pct_change()` function on either the `DataFrame` or a `Series`. This function will calculate the percentage change from the immediately preceding row by default. In our case with time series data, this is the action we want. In addition to finding the percentage change, we are also going to create a column that ranks the percentage change. The ranking column allows us to sort the data more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the percent change for volume and rank it\n",
    "# This returns a new DataFrame of the 10 largest\n",
    "# volume changes in percentage change from the previous trading day\n",
    "apple.assign(vol_pct_change = apple.volume.pct_change(),\n",
    "            pct_change_rank=lambda x: x.vol_pct_change.abs().rank(ascending=False)\n",
    "            ).nsmallest(10, 'pct_change_rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39662836",
   "metadata": {},
   "source": [
    "Now we want to try to create bins or buckets for the daily trading volume. If you want the ranges defining the bins to be equidistant, then you can use the `pd.cut()` function. Let's try breaking the column `volume` into three equidistant bins. We can use the argument `bins=3` and `pandas` will find the break points for the bins for us. You can optionally label each bin, which we will do as \"low\", \"medium\", and \"high\". If you want both the binned data as well as the ranges, you use the argument `retbins=True`. The first element of the returned tuple will be the binned data and the second element will contain the defining break points for the bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56881fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the returned tuple\n",
    "# First element is the data that has been binned\n",
    "# Second element is the break points of the bins\n",
    "vol_binned, bin_ranges = pd.cut(apple.volume, bins=3,\n",
    "                                labels=['low','medium','high'],\n",
    "                                retbins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the break points for the bins\n",
    "bin_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96475389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the range for each of 3 bins\n",
    "for i in range(len(bin_ranges)-1):\n",
    "    print(f'size of bin is: {bin_ranges[i+1] - bin_ranges[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c48ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many rows are in each bucket\n",
    "vol_binned.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the 13 high volume days\n",
    "apple[vol_binned=='high'].sort_values('volume', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156b98a8",
   "metadata": {},
   "source": [
    "Another way to create buckets is to try to have the same number of observations in each bucket. Doing so necessitates that the breakpoints for the bins **not** be equidistant. We can accomplish breaking a continuous variable into a discretized variable with each category containing (roughly) the same number of observations with the `pd.qcut()` function. The `q` stands for **quantile**. Using the argument `q=4` will give you back the quartiles; using `q=10` will return deciles. Let's break the `volume` variable into quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c02cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal number of observations in each bin\n",
    "# Unpack the returned tuple\n",
    "# First element is the data that has been binned\n",
    "# Second element is the break points of the bins\n",
    "vol_qbinned, qbin_ranges = pd.qcut(apple.volume, q=4,\n",
    "                                   labels=['q1','q2','q3','q4'],\n",
    "                                   retbins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many in each bin? Should be approximately same\n",
    "vol_qbinned.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c67511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the break points for qcut\n",
    "qbin_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the range for each of 4 bins (quartiles)\n",
    "for i in range(len(qbin_ranges)-1):\n",
    "    print(f'size of bin is: {qbin_ranges[i+1] - qbin_ranges[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d444662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call .describe on the volume\n",
    "# The 5 numbers for the qcut should be readily apparent\n",
    "apple.volume.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6aabf2",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "## Summarizing `DataFrame`s\n",
    "\n",
    "There are times when we want to summarize various columns of a `DataFrame`. Sometimes you want the same summary statistics for all the numerical columns. Other times, you might want different summary statistics for different columns. If you want the same summary statistic, e.g., the average of each column, you can use the `.apply()` function discussed above. Remember that each column data type must be in a form that allows the desired function to complete its action. If you want to find different summary statistics for different columns, the `.agg()` function can help. \n",
    "\n",
    "Because we added text columns to our `apple` dataset, calling `apple.apply(np.mean)` or something similar will fail. So, let's subset our `apple` dataset and then try using `.apply()` on the entire `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82656d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy with alias np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the output format for floats to be 2 decimals\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f390ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of nonnumerical columns\n",
    "only_num_apple = apple.select_dtypes(include=np.number)\n",
    "only_num_apple.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55c3169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average for all columns with .apply()\n",
    "only_num_apple.apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ea72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use .agg to find different summary statistics\n",
    "# for different columns. Because picking out columns\n",
    "# you can use the apple dataset\n",
    "apple.agg({'open':np.mean, 'high':np.max, 'low':np.min,\n",
    "          'close':np.mean, 'volume':np.sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872348a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to use more than one statistic\n",
    "# for a column, pass them as a list\n",
    "apple.agg({'open':'mean',\n",
    "          'high':['min','max'],\n",
    "          'low':['min','max'],\n",
    "          'close':'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0363f7d3",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "## Aggregating by Group\n",
    "\n",
    "There are many instances where you want to calculate aggregations per group. To do so, we can use the `.groupby()` function, passing in the column(s) we want to use to determine distinct groups. Let's add the trading volume groups to our `apple` dataset. We will use the `pd.cut()` function as we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the binned groups to the data\n",
    "apple = apple.assign(vol_group = pd.cut(apple.volume, bins=3,\n",
    "                   labels=['low','medium','high']))\n",
    "\n",
    "apple.info()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15161520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the average for each  vol_group\n",
    "apple.groupby('vol_group').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf0ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also pick out one of the columns from the resulting\n",
    "# groupby and do specific aggregations on that column\n",
    "apple.groupby('vol_group')['close'].agg(['min','max','mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a853481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to pull out different summary statistics\n",
    "# for different columns, you can use .agg\n",
    "app_agg = apple.groupby('vol_group').agg({\n",
    "    'open':'mean', 'high':['min','max'],\n",
    "    'low':['min','max'], 'close':'mean'})\n",
    "\n",
    "app_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d359b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the columns of app_agg\n",
    "app_agg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to \"flatten\" the columns, one approach\n",
    "# is to use list comprehension as shown here\n",
    "app_agg.columns = ['_'.join(col_agg) for col_agg in app_agg.columns]\n",
    "app_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd42d8f",
   "metadata": {},
   "source": [
    "You can also use a `pd.Grouper`, which allows you to specify a groupby instruction for an object. This is especially useful when you have an index that is a date. Our `apple` dataset has a date for an index. Suppose we wanted to find the average for the open, high, low, and close stock price for each month. This is easily accomplished with a `Grouper` by specifying the frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61bb92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the aveage open, high, low, and close price for each month\n",
    "apple.groupby(pd.Grouper(freq='M'))[['open','high','low','close']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865f4214",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "## Pivot Tables and Crosstabs\n",
    "\n",
    "You are probably familiar with the powerful concept of pivot tables from Excel. We can quickly generate pivot tables or cross tabulations in a commonly used format with `pandas`. You need to specify what to group on and, optionally, which subset of columns we want to aggregate and/or how to aggregate. The default aggregation is **average**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39038d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call pivot_table with a grouping vol_group\n",
    "apple.pivot_table(columns='vol_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c326d7",
   "metadata": {},
   "source": [
    "If you want the transpose of this pivot table, you can specify `index='vol_group'` instead. Let's see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc855a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the vol_group categories on the rows\n",
    "apple.pivot_table(index='vol_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167285e6",
   "metadata": {},
   "source": [
    "The function `pd.crosstab()` can be used to create a frequency table. For example, suppose we want to know how many low-, medium-, and high-volume trading days occurred in each month of our dataset. You use `index` to specify the rows and `columns` to specify the columns. By default, the values in the cells will be the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1cd0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency table\n",
    "# Rows will be the vol_group\n",
    "# Columns will be the month number\n",
    "pd.crosstab(index=apple.vol_group, columns=apple.index.month,\n",
    "           colnames=['month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f0495b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you want totals for the rows and columns\n",
    "# you can use the argument margins=True\n",
    "pd.crosstab(index=apple.vol_group, columns=apple.index.month,\n",
    "           colnames=['month'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b923ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want percentages by the month, you can use\n",
    "# the argument normalize='columns'\n",
    "pd.crosstab(index=apple.vol_group, columns=apple.index.month,\n",
    "           colnames=['month'], normalize='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b21078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want the overall percentage for each group over \n",
    "# the year, you can use normalize='columns' and margins=True\n",
    "pd.crosstab(index=apple.vol_group, columns=apple.index.month,\n",
    "           colnames=['month'], normalize='columns', margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda6e66f",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "The following links point you to additional resources that you might find helpful in learning this material.\n",
    "\n",
    "1. [The official API reference for `pandas.DataFrame.query`][1].\n",
    "2. [The official API reference for `pandas.Series.pct_change`][2].\n",
    "3. [The official API reference for `pandas.cut`][3].\n",
    "4. [The official API reference for `pandas.qcut`][4].\n",
    "5. [The official API reference for `pandas.Grouper`][5].\n",
    "\n",
    "-----\n",
    "\n",
    "[1]: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html\n",
    "[2]: https://pandas.pydata.org/docs/reference/api/pandas.Series.pct_change.html\n",
    "[3]: https://pandas.pydata.org/docs/reference/api/pandas.cut.html\n",
    "[4]: https://pandas.pydata.org/docs/reference/api/pandas.qcut.html\n",
    "[5]: https://pandas.pydata.org/docs/reference/api/pandas.Grouper.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55184a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "-----\n",
    "\n",
    "**&copy; 2022 - Present: Matthew D. Dean, Ph.D.   \n",
    "Clinical Associate Professor of Business Analytics at William \\& Mary.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
