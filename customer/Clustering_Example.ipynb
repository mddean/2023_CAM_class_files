{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94432801",
   "metadata": {},
   "source": [
    "# $K$-Means Clustering Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c879f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e98bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "cust = pd.read_excel('./data/small_customer_data.xlsx')\n",
    "cust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6506e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see info\n",
    "cust.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d3b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot with age on x-axis and income on y-axis\n",
    "sns.relplot(data=cust, x='Age', y='Income', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import k-means estimator\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff65705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many clusters do you want to try?\n",
    "first_attempt = KMeans(n_clusters=????)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfe0b58",
   "metadata": {},
   "source": [
    "We can use the method `fit_predict()` to compute the cluster centers and predict the cluster index for each sample. This method will return the labels or index of the cluster that each sample belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658bb1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and predict\n",
    "first_attempt.fit_predict(cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ed6672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To explicitly see the labels we have the atribute .labels_\n",
    "first_attempt.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49786c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To explicitly see the cluster centers we have the attribute\n",
    "# .cluster_centers_ which is a multidimensional array\n",
    "first_attempt.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff8a4d",
   "metadata": {},
   "source": [
    "Because we only have two attributes/variables we can easily plot the clusters. We will give each cluster a different color. Additionally, let's add the cluster centers to plot to see if they look like the center of their respective clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c34014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot\n",
    "# hue will be the labels_\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(x=cust.Age, y=cust.Income, hue=first_attempt.labels_, palette='tab10')\n",
    "\n",
    "# Now add the cluster centers to the plot\n",
    "# These will be triangles and colored black\n",
    "ax.scatter(first_attempt.cluster_centers_[:,0],\n",
    "           first_attempt.cluster_centers_[:,1],\n",
    "           marker='^', c='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c3cfac",
   "metadata": {},
   "source": [
    "## Notice Anything?\n",
    "\n",
    "What did you notice about the resulting clusters? Why do you think that is happening?\n",
    "\n",
    "### Solution?\n",
    "\n",
    "Let's try to scale the data. This will help each attribute start out on \"the same footing\" when trying to determine the clusters.\n",
    "\n",
    "There are various ways to scale the data. Depending on your chosen methodological approach, one type of scaling may be better than another. Recall that $K$-means clustering uses **distance** to determine the clusters. Because of this algorithmic detail, we want the range of the attributes to be the same. The easiest way to accomplish this task is with min-max scaling using the `MinMaxScaler`. \n",
    "\n",
    "One question you should always ask about any scaler is: Does the scaling change the original **shape** of my data?\n",
    "\n",
    "Let's see the reults of using the `MinMaxScaler` on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a histogram of `Age`\n",
    "sns.displot(cust.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5903e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a kernel density estimation plot\n",
    "sns.kdeplot(cust.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ace06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at histogram of Income\n",
    "sns.displot(cust.Income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e1575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE plot for Income\n",
    "sns.kdeplot(cust.Income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07539e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income looks right-skewed. Let's calculate\n",
    "# the skewness coefficient to verify\n",
    "cust.Income.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b7504",
   "metadata": {},
   "source": [
    "### Time to Scale\n",
    "\n",
    "We need to import `MinMaxScaler` from `sklearn.preprocessing`. Then we create an object by calling its class instantiator. We then fit and transform the data, make sure the columns are the same names as the original data, and then take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458e52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_scaler = MinMaxScaler()\n",
    "mm_scaled_data = pd.DataFrame(mm_scaler.fit_transform(cust))\n",
    "mm_scaled_data.columns = cust.columns\n",
    "mm_scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did Age change shape?\n",
    "sns.displot(mm_scaled_data.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa8e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE plot of Age\n",
    "sns.kdeplot(mm_scaled_data.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da76ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about Income\n",
    "sns.displot(mm_scaled_data.Income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a8d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE plot for Income\n",
    "sns.kdeplot(data=mm_scaled_data, x='Income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about the skewness of Income\n",
    "mm_scaled_data.Income.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af89dc88",
   "metadata": {},
   "source": [
    "### Result?\n",
    "\n",
    "Using the `MinMaxScaler` does **not** change the shape of our original data. \n",
    "\n",
    "### Find Clusters with Scaled Data\n",
    "\n",
    "Using the same number of clusters as above, we now want to fit and predict on the scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40841f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same number of clusters as above\n",
    "second_attempt = KMeans(n_clusters=????)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the labels for each observation\n",
    "# It will return the labels_ \n",
    "second_attempt.fit_predict(mm_scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do the cluster centers look like for the scaled data?\n",
    "second_attempt.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf653d",
   "metadata": {},
   "source": [
    "### Centers on Scaled Data\n",
    "\n",
    "Notice that the cluster centers are in terms of the scaled data. This fact should not surpise you: you sent in scaled data, so the centers necessarily must also be scaled from the original data. Because the cluster centers are on a different scale than the original data, how do we interpret them in the original scale? For example, for the first cluster center, how do you interpret the Age attribute that shows, say 0.8153? To make sense of the cluster centers we need to put them back into the original units. Luckily, this is easily accomplished with the `.inverse_transform()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the new centers back into original units\n",
    "centers_orig_scale = mm_scaler.inverse_transform(second_attempt.cluster_centers_)\n",
    "centers_orig_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a217d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, plot the new clusters and their centers\n",
    "# Notice, that we can use the original data for the x and y coordinates\n",
    "# and simply change the color of the observations based on the labels\n",
    "# that resulted from clustering with the scaled data\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(x=cust.Age, y=cust.Income, hue=second_attempt.labels_, palette='tab10')\n",
    "\n",
    "# To correctly get the centers on the chart, we need to use the\n",
    "# inverse_transform'ed centers\n",
    "ax.scatter(centers_orig_scale[:,0],\n",
    "           centers_orig_scale[:,1],\n",
    "           marker='^', c='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef6e10a",
   "metadata": {},
   "source": [
    "## Finding the \"Right\" Number of Clusters\n",
    "\n",
    "How do you know the \"correct\" number of clusters to use? In some instances, the number is solely driven by the business context. For example, you want to 3 groups of customers so that you can target each of those 3 groups separately. The business owner specified they wanted 3 groups. Other times, you may let the algorithm(s) decide the number of clusters. \n",
    "\n",
    "One approach to finding a \"good\" number of clusters is called the \"elbow method\". In this approach, you try different values for $k$ and plot the resulting values of *inertia*. You then try to find the \"elbow\" where adding an additional cluster does not drastically improve (i.e., lower) the inertia.\n",
    "\n",
    "Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fd1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to hold the results\n",
    "# key = the number of clusters : value = inertia\n",
    "inertia = {}\n",
    "\n",
    "# Loop over 1 to 9 clusters finding the clusters and inertia\n",
    "for k_value in range(1,10):\n",
    "    kmeans = KMeans(n_clusters=k_value, random_state=42)\n",
    "    kmeans.fit(mm_scaled_data)\n",
    "    inertia[k_value] = kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d0056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at results\n",
    "inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3281bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "# Create DataFrame for easier plotting\n",
    "df_inertia = pd.DataFrame.from_dict(inertia, orient='index', columns=['inertia'])\n",
    "df_inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inertia.plot(marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5fdf82",
   "metadata": {},
   "source": [
    "Another approach instead of the elbow method is to calculate the **silhoutte score** for each value of $k$ to see how many clusters should be used. The best value is +1 and the worst is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar. A +1 indicates highly dense clustering. Overall, you can think of the following: the score is higher when the clusters are dense and well separated, which relates to a standard concept of a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import silhouette_score\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to hold silhouette scores\n",
    "ss = {}\n",
    "\n",
    "# Loop over 2 to 9 clusters finding the clusters and silhouette_score\n",
    "for k_value in range(2,10):\n",
    "    kmeans = KMeans(n_clusters=k_value, random_state=42)\n",
    "    kmeans.fit(mm_scaled_data)\n",
    "    labels = kmeans.labels_\n",
    "    ss[k_value] = silhouette_score(mm_scaled_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65399d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the results look like\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to make plotting easier\n",
    "df_ss = pd.DataFrame.from_dict(ss, orient='index', columns=['silhouette_score'])\n",
    "df_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adea99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot\n",
    "df_ss.plot(marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c858e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try creating clusters with the \"best\" number indicated above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957df868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the new centers back into original units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, plot the new clusters and their centers\n",
    "# Notice, that we can use the original data for the x and y coordinates\n",
    "# and simply change the color of the observations based on the labels\n",
    "# that resulted from clustering with the scaled data\n",
    "\n",
    "\n",
    "# To correctly get the centers on the chart, we need to use the\n",
    "# inverse_transform'ed centers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e3750",
   "metadata": {},
   "source": [
    "## Another Scaling Option\n",
    "\n",
    "Another popular scaling option is the `StandardScaler`. It standardizes the data; that is, the scaled data will have a mean of 0 and a variance of 1. (Obviously, it will also have a standard deviation of 1.) This scaler is often used in many machine learning estimators that assume that the individual attributes/variables/features look more or less like a standard normally distributed distribution.\n",
    "\n",
    "I will leave it to you to explore the scaler on your own using the ancillary material below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd9c0f",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "\n",
    "The following links point you to additional resources that you might find helpful in learning this material.\n",
    "\n",
    "1. [API documentation for `KMeans`][1].\n",
    "2. [API documentation for `MinMaxScaler`][2].\n",
    "3. [API documentation for `StandardScaler`][3].\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "[1]: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "[2]: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "[3]: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045a847",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2023 - Present: Matthew D. Dean, Ph.D.   \n",
    "Clinical Associate Professor of Business Analytics at William \\& Mary.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
