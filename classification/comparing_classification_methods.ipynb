{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214a01d8",
   "metadata": {},
   "source": [
    "# Comparing Classification Models\n",
    "\n",
    "We want to compare various classification models for the customer dataset. We'll look at logistic regression, LDA, QDA, and $k$-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We'll look at logistic regression, LDA, QDA, and KNN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# We'll standardize and split data into training/testing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Need to measure \"goodness\"\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, recall_score, average_precision_score, precision_score\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, PrecisionRecallDisplay, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data and print out its shape\n",
    "cust = pd.read_csv('./data/customers_clean.csv')\n",
    "print(cust.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d235b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop the following columns:\n",
    "# cust_id, join_date, last_purchase_date\n",
    "new_cust = cust.drop(columns=['cust_id','join_date','last_purchase_date'])\n",
    "new_cust.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies and save in new DataFrame\n",
    "data = pd.get_dummies(new_cust, dtype=int, drop_first=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed07b8",
   "metadata": {},
   "source": [
    "## Create \"big spender\"\n",
    "\n",
    "We want to convert the $y$ variable, `spend`, into a binary variable where 1 represents a big spender and a 0 otherwise. We can set the cutoff anywhere we want. Looking at the summary statistics from above, let's use \\$4,700 (what you told me to do) as the cutoff. We can use the function `pd.cut()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd67fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['big_spender'] = pd.cut(data.spend, bins=[0,4700,10000], right=True, labels=[0,1]).astype(int)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b8b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See summary statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d22025",
   "metadata": {},
   "source": [
    "## Split Data into Training and Test Sets\n",
    "\n",
    "Need to first define `X` and `y`. Then we can try train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c547fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the output variable, y\n",
    "y = data.big_spender\n",
    "\n",
    "# define the X\n",
    "# Notice this time we are only dropping the original 'spend'\n",
    "# and the 'big_spender' columns.\n",
    "X = data.drop(columns=['spend', 'big_spender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=163)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the percentage of big spenders in both training and test?\n",
    "print(f'Training percentage of big spenders is {y_train.mean():.2%}')\n",
    "print(f'Testing percentage of big spenders is  {y_test.mean():.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca8cc1",
   "metadata": {},
   "source": [
    "## Scale the Data\n",
    "\n",
    "Fit only on the training set. Use that fit to transform both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ac0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler on just the training X variables\n",
    "# Let's start with StandardScaler which will center\n",
    "# each variable at 0 and give each a unit variance (=1)\n",
    "s_scaler = StandardScaler().fit(X_train)\n",
    "s_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec9747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train and put in DataFrame\n",
    "X_train_ss = pd.DataFrame(s_scaler.transform(X_train), columns=X_train.columns)\n",
    "\n",
    "# Take a look at the DataFrame\n",
    "X_train_ss.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c35e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_test and put in DataFrame\n",
    "X_test_ss = pd.DataFrame(s_scaler.transform(X_test), columns=X_test.columns)\n",
    "X_test_ss.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545ff37",
   "metadata": {},
   "source": [
    "# Function to Look at Different Models\n",
    "\n",
    "I have write a custom function that will take in different models (classifiers) and compute the different metrics so we can compare the different classifiers easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce4fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our function\n",
    "def modelMetrics(classifier, name, X_test, y_test):\n",
    "    \"\"\"\n",
    "    We want to see how the different models react to the same \n",
    "    dataset. We should capture multiple metrics for each model.\n",
    "    \n",
    "    classifier: the classifier we are capturing metrics for\n",
    "    \n",
    "    name: give it a descriptive name\n",
    "    \n",
    "    X_test: the X array for the test set\n",
    "    \n",
    "    y_test: the output variable (actual) for test set\n",
    "    \"\"\"\n",
    "    retVal = {}\n",
    "    \n",
    "    metrics = {}\n",
    "    predictions = classifier.predict(X_test)\n",
    "    metrics['a_score'] = accuracy_score(y_test, predictions)\n",
    "    metrics['r_score'] = recall_score(y_test, predictions)\n",
    "    metrics['p_score'] = precision_score(y_test, predictions)\n",
    "    metrics['f1_score'] = f1_score(y_test, predictions)\n",
    "    metrics['f2_score'] = fbeta_score(y_test, predictions, beta=2)\n",
    "    metrics['f0.5_score'] = fbeta_score(y_test, predictions, beta=0.5)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    totPositives = y_test.sum()\n",
    "    totNegatives = len(y_test) - totPositives\n",
    "    \n",
    "    # Error rate negatives = false positives / total negatives\n",
    "    metrics['errorNegatives'] = fp/totNegatives\n",
    "    # Accuracy for negatives = true negatives / total negatives\n",
    "    metrics['accNegatives'] = tn/totNegatives\n",
    "    # Error rate for positives = false negatives / total positives\n",
    "    metrics['errorPositives'] = fn/totPositives\n",
    "    # Accuracy for positives = true positives / total positives\n",
    "    metrics['accPositives'] = tp/totPositives\n",
    "\n",
    "    metrics['roc_auc_score'] = roc_auc_score(y_test,\n",
    "                                             classifier.predict_proba(X_test)[:,1])\n",
    "    metrics['avg_p_score'] = average_precision_score(y_test,\n",
    "                                                     classifier.predict_proba(X_test)[:,1])\n",
    "    \n",
    "    retVal[name] = metrics\n",
    "    \n",
    "    return pd.DataFrame(retVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020ee7e",
   "metadata": {},
   "source": [
    "# Create a Logistic Model\n",
    "\n",
    "We will use the scaled data and include all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb1eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LogisticRegression\n",
    "logReg = LogisticRegression()\n",
    "\n",
    "# fit the logistic regression model\n",
    "logReg.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea160d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the estimated intercept and coefficients\n",
    "print(logReg.intercept_)\n",
    "print(logReg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b18912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model metrics using user-defined function\n",
    "lr_metrics = modelMetrics(logReg, 'logistic regression', X_test_ss, y_test)\n",
    "lr_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a plot of confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(logReg, X_test_ss, y_test, cmap='cividis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93795e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ROC Curve display\n",
    "RocCurveDisplay.from_estimator(logReg, X_test_ss, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231fd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Precision Recall display\n",
    "PrecisionRecallDisplay.from_estimator(logReg, X_test_ss, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a12caf0",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis\n",
    "Let's now try LDA. We already imported the appropriate packages for LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e502ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LDA instance\n",
    "lda = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the LDA model with X and y training set created previously\n",
    "lda.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5fb436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model metrics using user-defined function\n",
    "lda_metrics = modelMetrics(lda, 'LDA', X_test_ss, y_test)\n",
    "lda_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae170418",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis\n",
    "We can also try QDA (already imported package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c335800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a QDA instance\n",
    "qda = QuadraticDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c44256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the qda with X and y\n",
    "qda.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a9ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model metrics using user-defined function\n",
    "qda_metrics = modelMetrics(qda, 'QDA', X_test_ss, y_test)\n",
    "qda_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158617f",
   "metadata": {},
   "source": [
    "# KNN\n",
    "Might as well try $k$-nearest neighbors. You pick $k$, so it will be your fault not mine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d26c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use k=??\n",
    "knn = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "knn.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4bf725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model metrics using user-defined function\n",
    "knn3_metrics = modelMetrics(knn, 'KNN-3', X_test_ss, y_test)\n",
    "knn3_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d598b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try another value for k ... k=??\n",
    "knn20 = KNeighborsClassifier(n_neighbors=20)\n",
    "knn20.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f84d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model metrics using user-defined function\n",
    "knn20_metrics = modelMetrics(knn20, 'KNN-20', X_test_ss, y_test)\n",
    "knn20_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2273d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all metrics in one DataFrame to examine\n",
    "all_dfs = [lr_metrics, lda_metrics, qda_metrics, knn3_metrics, knn20_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291146fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at all metrics together\n",
    "all_metrics = pd.concat(all_dfs, axis=1)\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_metrics.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f76f614",
   "metadata": {},
   "source": [
    "## Find the \"Best\" $k$ for KNN\n",
    "\n",
    "We can systematically search various values of $k$ to find the one that gives us the best performance (will need to define) using `GridSearchCV`. What `GridSearchCV` does is train the model multiple times on a range of parameters we specify. In our case that would be $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499fbbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b48206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new knn model\n",
    "knn_base = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05151e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our search grid\n",
    "# This is a dictionary of values for k we want search thru\n",
    "param_grid = {'n_neighbors': np.arange(1, 25)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the grid search\n",
    "knnCV = GridSearchCV(knn_base, param_grid, cv=5, scoring='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85093e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "knnCV.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d8aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "knnCV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c243c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "knnCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27435946",
   "metadata": {},
   "outputs": [],
   "source": [
    "knnCV.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the grid search\n",
    "knnCV = GridSearchCV(knn_base, param_grid, cv=5, scoring='average_precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487d9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "knnCV.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315287ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "knnCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367cc0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "knnCV.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f8f554",
   "metadata": {},
   "source": [
    "## Threshold-Moving\n",
    "\n",
    "So far, we've been just keeping the default cut-off probability, which happens to be 0.5, for determining whether our binary class should be a 0 (non-big spender) or a 1 (big spender). We can change the **threshold** for what is considered class 0 or 1. This is called **threshold-moving**.\n",
    "\n",
    "Let's change the threshold from the default of 0.5 to 0.2 for the LDA model and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e82ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function that will do all the hard work for us (hopefully)\n",
    "def compare_metrics(classifier, X_test, y_test):\n",
    "    results = {}\n",
    "#     retVal = []\n",
    "    # Loop over different thresholds\n",
    "    for threshold in np.arange(0.1, 0.91, 0.01):\n",
    "        yProb = classifier.predict_proba(X_test)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, yProb[:,1]>threshold).ravel()\n",
    "        totPositives = y_test.sum()\n",
    "        totNegatives = len(y_test) - totPositives\n",
    "\n",
    "        # Accuracy for negatives = true negatives / total negatives\n",
    "        accNegatives = tn/totNegatives\n",
    "        # Accuracy for positives = true positives / total positives\n",
    "        accPositives = tp/totPositives\n",
    "        \n",
    "        results[threshold] = [accPositives, accNegatives]\n",
    "#         # Add result to retVal list\n",
    "#         retVal.append((threshold, accPositives, accNegatives))\n",
    "        retVal = pd.DataFrame.from_dict(results, orient='index', columns=['Accuracy Big Spenders','Accuracy Others'])\n",
    "        \n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3663d1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try it with logistic regression\n",
    "values = compare_metrics(logReg, X_test_ss, y_test)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9018abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(values.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3cf218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The scope of these changes made to\n",
    "# pandas settings are local to with statement.\n",
    "with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.precision', 3,\n",
    "                       ):\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0646c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(values.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57df87c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=values.index, y='Accuracy Big Spenders', data=values)\n",
    "sns.relplot(x=values.index, y='Accuracy Others', data=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c66cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(values.index, y=values['Accuracy Big Spenders'], label='big spender accuracy')\n",
    "ax.scatter(values.index, values['Accuracy Others'], label='other accuracy', marker='s', color='orange')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Cut-off value')\n",
    "ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47081bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c569c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134dcbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285111a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
