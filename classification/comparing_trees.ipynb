{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214a01d8",
   "metadata": {},
   "source": [
    "# Comparing Classification Models\n",
    "\n",
    "We want to compare various classification models for the customer dataset. We'll look at logistic regression, LDA, QDA, and $k$-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We'll look at logistic regression, LDA, QDA, and KNN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Need to import tree stuff\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz, export_text, plot_tree\n",
    "\n",
    "# Ensemble methods for trees\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "# We'll standardize and split data into training/testing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import RFECV class\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Need to measure \"goodness\"\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, recall_score, average_precision_score, precision_score\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, PrecisionRecallDisplay, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data and print out its shape\n",
    "cust = pd.read_csv('./data/customers_clean.csv')\n",
    "print(cust.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d235b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop the following columns:\n",
    "# cust_id, join_date, last_purchase_date\n",
    "new_cust = cust.drop(columns=['cust_id','join_date','last_purchase_date'])\n",
    "new_cust.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies and save in new DataFrame\n",
    "data = pd.get_dummies(new_cust, dtype=int, drop_first=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed07b8",
   "metadata": {},
   "source": [
    "## Create \"big spender\"\n",
    "\n",
    "We want to convert the $y$ variable, `spend`, into a binary variable where 1 represents a big spender and a 0 otherwise. We can set the cutoff anywhere we want. Looking at the summary statistics from above, let's use \\$4,700 (what you told me to do) as the cutoff. We can use the function `pd.cut()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd67fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['big_spender'] = pd.cut(data.spend, bins=[0,5700,10000], right=True, labels=[0,1]).astype(int)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b8b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See summary statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d22025",
   "metadata": {},
   "source": [
    "## Split Data into Training and Test Sets\n",
    "\n",
    "Need to first define `X` and `y`. Then we can try train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c547fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the output variable, y\n",
    "y = data.big_spender\n",
    "\n",
    "# define the X\n",
    "# Notice this time we are only dropping the original 'spend'\n",
    "# and the 'big_spender' columns.\n",
    "X = data.drop(columns=['spend', 'big_spender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=163)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the percentage of big spenders in both training and test?\n",
    "print(f'Training percentage of big spenders is {y_train.mean():.2%}')\n",
    "print(f'Testing percentage of big spenders is  {y_test.mean():.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca8cc1",
   "metadata": {},
   "source": [
    "## Scale the Data\n",
    "\n",
    "Fit only on the training set. Use that fit to transform both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ac0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler on just the training X variables\n",
    "# Let's start with StandardScaler which will center\n",
    "# each variable at 0 and give each a unit variance (=1)\n",
    "s_scaler = StandardScaler().fit(X_train)\n",
    "s_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec9747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train and put in DataFrame\n",
    "X_train_ss = pd.DataFrame(s_scaler.transform(X_train), columns=X_train.columns)\n",
    "\n",
    "# Take a look at the DataFrame\n",
    "X_train_ss.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c35e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_test and put in DataFrame\n",
    "X_test_ss = pd.DataFrame(s_scaler.transform(X_test), columns=X_test.columns)\n",
    "X_test_ss.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545ff37",
   "metadata": {},
   "source": [
    "# Function to Look at Different Models\n",
    "\n",
    "I have write a custom function that will take in different models (classifiers) and compute the different metrics so we can compare the different classifiers easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce4fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our function\n",
    "def modelMetrics(classifier, name, X_test, y_test):\n",
    "    \"\"\"\n",
    "    We want to see how the different models react to the same \n",
    "    dataset. We should capture multiple metrics for each model.\n",
    "    \n",
    "    classifier: the classifier we are capturing metrics for\n",
    "    \n",
    "    name: give it a descriptive name\n",
    "    \n",
    "    X_test: the X array for the test set\n",
    "    \n",
    "    y_test: the output variable (actual) for test set\n",
    "    \"\"\"\n",
    "    retVal = {}\n",
    "    \n",
    "    metrics = {}\n",
    "    predictions = classifier.predict(X_test)\n",
    "    metrics['a_score'] = accuracy_score(y_test, predictions)\n",
    "    metrics['r_score'] = recall_score(y_test, predictions)\n",
    "    metrics['p_score'] = precision_score(y_test, predictions)\n",
    "    metrics['f1_score'] = f1_score(y_test, predictions)\n",
    "    metrics['f2_score'] = fbeta_score(y_test, predictions, beta=2)\n",
    "    metrics['f0.5_score'] = fbeta_score(y_test, predictions, beta=0.5)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    totPositives = y_test.sum()\n",
    "    totNegatives = len(y_test) - totPositives\n",
    "    \n",
    "    # Error rate negatives = false positives / total negatives\n",
    "    metrics['errorNegatives'] = fp/totNegatives\n",
    "    # Accuracy for negatives = true negatives / total negatives\n",
    "    metrics['accNegatives'] = tn/totNegatives\n",
    "    # Error rate for positives = false negatives / total positives\n",
    "    metrics['errorPositives'] = fn/totPositives\n",
    "    # Accuracy for positives = true positives / total positives\n",
    "    metrics['accPositives'] = tp/totPositives\n",
    "\n",
    "    metrics['roc_auc_score'] = roc_auc_score(y_test,\n",
    "                                             classifier.predict_proba(X_test)[:,1])\n",
    "    metrics['avg_p_score'] = average_precision_score(y_test,\n",
    "                                                     classifier.predict_proba(X_test)[:,1])\n",
    "    \n",
    "    retVal[name] = metrics\n",
    "    \n",
    "    return pd.DataFrame(retVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020ee7e",
   "metadata": {},
   "source": [
    "# Create a Logistic Model\n",
    "\n",
    "We will use the scaled data and include all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb1eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LogisticRegression\n",
    "logReg = LogisticRegression()\n",
    "\n",
    "# fit the logistic regression model\n",
    "logReg.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea160d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the estimated intercept and coefficients\n",
    "print(logReg.intercept_)\n",
    "print(logReg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b18912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model metrics using user-defined function\n",
    "lr_metrics = modelMetrics(logReg, 'logistic regression', X_test_ss, y_test)\n",
    "lr_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a plot of confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(logReg, X_test_ss, y_test, cmap='cividis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93795e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ROC Curve display\n",
    "RocCurveDisplay.from_estimator(logReg, X_test_ss, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231fd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Precision Recall display\n",
    "PrecisionRecallDisplay.from_estimator(logReg, X_test_ss, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaae12b",
   "metadata": {},
   "source": [
    "## Logistic Regression CV\n",
    "\n",
    "We can use `LogisticRegressionCV` to regularize the coefficients on the input variables to attempt to get a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11dfafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0433beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegCV = LogisticRegressionCV().fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ec209",
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegCV_metrics = modelMetrics(logRegCV, 'logistic regression CV', X_test_ss, y_test)\n",
    "logRegCV_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335c840",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "We can also use `RFECV` on a `LogisticRegression` to find the smallest set of input variables for a model. Let's try it with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a50e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg_rfecv = LogisticRegression()\n",
    "rfecv = RFECV(logReg_rfecv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg_rfecv.fit(rfecv.fit_transform(X_train_ss, y_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d63941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df8efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ss.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc72e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(rfecv, X_test_ss, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71137713",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_metrics = modelMetrics(rfecv, 'RFE logistic regression', X_test_ss, y_test)\n",
    "rfe_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a12caf0",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis\n",
    "Let's now try LDA. We already imported the appropriate packages for LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e502ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LDA instance\n",
    "lda = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the LDA model with X and y training set created previously\n",
    "lda.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5fb436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model metrics using user-defined function\n",
    "lda_metrics = modelMetrics(lda, 'LDA', X_test_ss, y_test)\n",
    "lda_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae170418",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis\n",
    "We can also try QDA (already imported package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c335800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a QDA instance\n",
    "qda = QuadraticDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c44256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the qda with X and y\n",
    "qda.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a9ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model metrics using user-defined function\n",
    "qda_metrics = modelMetrics(qda, 'QDA', X_test_ss, y_test)\n",
    "qda_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158617f",
   "metadata": {},
   "source": [
    "# KNN\n",
    "Might as well try $k$-nearest neighbors. Let's use $k=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d26c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use k=k\n",
    "knn = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "knn.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4bf725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model metrics using user-defined function\n",
    "knn3_metrics = modelMetrics(knn, 'KNN-3', X_test_ss, y_test)\n",
    "knn3_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a3c553",
   "metadata": {},
   "source": [
    "## A Simple Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ccb7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier().fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_metrics = modelMetrics(dt, 'Decision Tree', X_test_ss, y_test)\n",
    "dt_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 20))\n",
    "plot_tree(dt, feature_names=X.columns, filled=True, rounded=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbbfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt3.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 20))\n",
    "plot_tree(dt3, feature_names=X.columns, filled=True, rounded=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dedd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt3_metrics = modelMetrics(dt3, 'Decision Tree - 3', X_test_ss, y_test)\n",
    "dt3_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e35af",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({'Importance':dt3.feature_importances_*100},\n",
    "                          index=X.columns)\n",
    "importance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh')\n",
    "plt.xlabel('Variable Importance')\n",
    "plt.gca().legend_ = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aba5432",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c36fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = BaggingClassifier(random_state=42).fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571c6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_metrics = modelMetrics(bag, 'Bagging', X_test_ss, y_test)\n",
    "bag_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f6dac",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679116d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42).fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26aec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_metrics = modelMetrics(rf, 'Random Forest', X_test_ss, y_test)\n",
    "rf_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac418c0c",
   "metadata": {},
   "source": [
    "## Extra Trees (Extremely Randomized Trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3d046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtClf = ExtraTreesClassifier(random_state=42).fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae1f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_metrics = modelMetrics(xtClf, 'Extra Trees', X_test_ss, y_test)\n",
    "xt_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9b9ee",
   "metadata": {},
   "source": [
    "## Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(random_state=42, n_estimators=500)\n",
    "ada.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db5489",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_metrics = modelMetrics(ada, 'Adaptive Boosting', X_test_ss, y_test)\n",
    "ada_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4423a9ae",
   "metadata": {},
   "source": [
    "## Gradient Tree Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2172bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost = GradientBoostingClassifier(random_state=42, n_estimators=500, learning_rate=0.01)\n",
    "gboost.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f3bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost_metrics = modelMetrics(gboost, 'Gradient Boosting', X_test_ss, y_test)\n",
    "gboost_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a82bdc",
   "metadata": {},
   "source": [
    "## Histogram-based Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6d6c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hgb = HistGradientBoostingClassifier(random_state=42, learning_rate=0.01, max_iter=500)\n",
    "hgb.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb_metrics = modelMetrics(hgb, 'Hist Gradient Boosting', X_test_ss, y_test)\n",
    "hgb_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef4ee5a",
   "metadata": {},
   "source": [
    "## Extreme Gradient Boosting (XG Boost)\n",
    "\n",
    "We will probably need to install it first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd92e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd78b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbClf = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea08a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbClf.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94596d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_metrics = modelMetrics(xgbClf, 'XGB', X_test_ss, y_test)\n",
    "xgb_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c4d25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hgb = HistGradientBoostingClassifier(random_state=42, learning_rate=0.01, max_iter=500)\n",
    "hgb.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2b0542",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb_metrics = modelMetrics(hgb, 'Hist Gradient Boosting', X_test_ss, y_test)\n",
    "hgb_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2273d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all metrics in one DataFrame to examine\n",
    "all_dfs = [lr_metrics, lda_metrics, qda_metrics, knn3_metrics,\n",
    "           dt_metrics, dt3_metrics, bag_metrics, rf_metrics,\n",
    "           xt_metrics, ada_metrics, gboost_metrics, hgb_metrics, xgb_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291146fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at all metrics together\n",
    "all_metrics = pd.concat(all_dfs, axis=1)\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_metrics.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3cf218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The scope of these changes made to\n",
    "# pandas settings are local to with statement.\n",
    "with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.precision', 3,\n",
    "                       ):\n",
    "    print(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0646c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics.to_csv('all_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47081bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c569c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de16d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
